import streamlit as st
from collections import Counter
import pandas as pd
from pymongo import MongoClient
import json

#for word clound
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_stopwords
from pythainlp.corpus import common
from pythainlp.util import Trie
import re

st.title("YouTube Comments Analysis")
st.markdown("""
### üî∞ Introduction
Applying machine learning techniques to text classification. Its primary objective is to automatically categorize comments from YouTube car review videos into predefined categories, enabling deeper insights into public sentiment and user feedback.
"""
"""### üîç Data Source and Collection
Text Comments: Extracted from the comment sections of YouTube car review videos.  
Target Brand: BYD, with a focus on its three best-selling models in the Thai market ‚Äî Atto 3, Seal, and Dolphin.  
YouTube Channel: Data was collected from the official *autolifethailand* channel, a trusted automotive source with over 1.06 million subscribers. 
Comments were gathered from three BYD-related review videos, which form the basis for training and evaluating the classification model.
"""
"""### üí° AI-Powered Insight Assistant
To enhance the value of this project, an AI-powered assistant feature is included for premium users. This tool enables interactive exploration of classified comments, identifies key discussion points, highlights emerging trends, and provides contextual insight summaries‚Äîsupporting smarter and faster decision-making driven by public opinion.
""")

# === YouTube Video IDs ===
video_ids = ["OMV9F9zB4KU", "87lJCDADWCo", "CbkX7H-0BIU"]
# === Show Videos ===
st.subheader("‚ñ∂Ô∏è Video Reference üî¥")
col1, col2, col3 = st.columns(3)
with col1:
    st.video(f"https://www.youtube.com/watch?v={video_ids[0]}")
    st.caption("BYD Atto3")
with col2:
    st.video(f"https://www.youtube.com/watch?v={video_ids[1]}")
    st.caption("BYD Seal")
with col3:
    st.video(f"https://www.youtube.com/watch?v={video_ids[2]}")
    st.caption("BYD Dolphin")
# === Sidebar: Conversation History ===

# === MongoDB ===
mongo_uri = "mongodb+srv://readwrite:OSbtDM3XE8nP2JqT@voranitha.z6voe4w.mongodb.net/"
 
@st.cache_resource
def get_database():
    """
    ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö MongoDB Atlas ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô object ‡∏Ç‡∏≠‡∏á database
    ‡πÉ‡∏ä‡πâ st.cache_resource ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    """
    try:
        client = MongoClient(mongo_uri)
        client.admin.command('ping') # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        return client.car # ‡∏Ñ‡∏∑‡∏ô database 'car'
    except ConnectionFailure as e:
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö MongoDB ‡πÑ‡∏î‡πâ: {e}")
        st.stop() # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Streamlit ‡∏´‡∏≤‡∏Å‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
    except OperationFailure as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô MongoDB: {e}")
        st.stop()
    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {e}")
        st.stop()
 
# --- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Streamlit App ---
 
# ‡∏î‡∏∂‡∏á database object
db = get_database()
collection = db.comment
data = list(collection.find())  # Get all documents as a list of dicts
df = pd.DataFrame(data)         # Convert to DataFrame

# ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞ video_title
#comments = list(collection.find({}, {"video_title": 1}))
#comments = list(collection.find({}, {"_id": 0, "video_title": 1}))

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
#df = pd.DataFrame(comments)
#st.write(comments[:5]) 

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
if 'video_title' not in df.columns:
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'video_title'")
else:
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏ï‡πà‡∏≠ video_title
    video_counts = df['video_title'].value_counts().reset_index()
    video_counts.columns = ['video_title', 'count']

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á label ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    def generate_label(title):
        title_upper = title.upper()
        if "ATTO" in title_upper:
            return "BYD Atto3"
        elif "SEAL" in title_upper:
            return "BYD Seal"
        elif "DOLPHIN" in title_upper:
            return "BYD Dolphin"
        else:
            return "‡∏≠‡∏∑‡πà‡∏ô ‡πÜ"

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå label
    video_counts['Model'] = video_counts['video_title'].apply(generate_label)

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà
    result_df = video_counts[['Model', 'video_title', 'count']]

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    st.subheader("üìä Video Comment Counts")
    st.dataframe(result_df)

# Word Cloud
comments_collection = db.comment

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Dictionary ---
# ‡πÇ‡∏´‡∏•‡∏î‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á pythainlp
default_words = common.thai_words()
custom_word_list = list(default_words) + ["‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà", "‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà","‡∏ã‡∏∑‡πâ‡∏≠","‡πÄ‡∏á‡∏¥‡∏ô","‡πÄ‡∏™‡∏µ‡∏¢‡∏á","‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà","‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô","‡∏ä‡∏≤‡∏£‡πå‡∏à","‡∏Å‡∏•‡πâ‡∏≠‡∏á","‡πÄ‡∏ó‡∏Ñ","‡∏≠‡∏≠‡∏ü‡∏ä‡∏±‡πà‡∏ô","‡∏™‡∏µ","‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô","‡∏´‡∏ô‡πâ‡∏≤","‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå"] 
custom_dictionary = Trie(custom_word_list)
# ----------------------------------

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud ---
# --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mapping ‡∏ä‡∏∑‡πà‡∏≠ Model ‡∏Å‡∏±‡∏ö Keyword ‡πÉ‡∏ô title ---
model_keywords = {
    "BYD Atto3": "ATTO",
    "BYD Dolphin": "DOLPHIN",
    "BYD Seal": "SEAL"
}
st.header("‚òÅÔ∏è Word Cloud")
# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Dropdown ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ---
selected_model = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡πà‡∏ô‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π Word Cloud", list(model_keywords.keys()))

if selected_model: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏Ñ‡πà‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    try:
        keyword = model_keywords[selected_model]

        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ Query ‡∏à‡∏≤‡∏Å MongoDB ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ---
        cursor = comments_collection.find(
            {"video_title": {"$regex": keyword, "$options": "i"}},
            {"comment": 1}
        )
        # ---------------------------------------------------

        # ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        comments = [re.sub(r"[^\u0E00-\u0E7F\s]+", " ", doc.get("comment", "")) for doc in cursor]
        full_text = " ".join(comments)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not full_text.strip():
            st.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∏‡πà‡∏ô **{selected_model}** ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î **'{keyword}'**")
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á st.stop() ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ dropdown ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
        else: # ‡πÄ‡∏û‡∏¥‡πà‡∏° else block ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Word Cloud ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Custom Dictionary
            tokens = word_tokenize(full_text, engine="newmm", custom_dict=custom_dictionary)

            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Stopwords ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            combined_stopwords = set(thai_stopwords()).union({
                "‡∏Ñ‡∏£‡∏±‡∏ö","‡∏Ñ‡πà‡∏∞","‡πÜ","‡∏ô‡∏∞","‡πÄ‡∏•‡∏¢","‡πÄ‡∏õ‡πá‡∏ô","‡∏Ñ‡∏∑‡∏≠","‡∏ß‡πà‡∏≤","‡πÑ‡∏î‡πâ","‡∏à‡∏∞","‡∏Å‡πá","‡πÅ‡∏•‡πâ‡∏ß","‡∏°‡∏≤‡∏Å","‡∏ô‡πâ‡∏≠‡∏¢","‡πÑ‡∏õ","‡∏°‡∏≤","‡∏≠‡∏¢‡∏π‡πà","‡∏°‡∏µ","‡πÑ‡∏°‡πà","‡∏Ñ‡∏ô","‡∏Ñ‡∏±‡∏ô","‡∏ï‡∏±‡∏ß","‡∏£‡∏∏‡πà‡∏ô","‡∏ó‡∏≥‡πÑ‡∏°","‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ","‡πÄ‡∏´‡πá‡∏ô","‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô","‡πÉ‡∏ô","‡∏£‡∏π‡∏õ","‡∏ñ‡∏π‡∏Å"
            })

            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Allowed Words (‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
            allowed_words = {
                "‡∏î‡∏µ", "‡πÅ‡∏£‡∏á", "‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î", "‡∏™‡∏ß‡∏¢", "‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å", "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏Ñ‡∏∏‡πâ‡∏°", "‡∏™‡∏ö‡∏≤‡∏¢", "‡πÄ‡∏£‡πá‡∏ß", "‡πÄ‡∏á‡∏µ‡∏¢‡∏ö",
                "‡∏ã‡∏∑‡πâ‡∏≠", "‡∏Ç‡∏≤‡∏¢", "‡∏ñ‡∏π‡∏Å", "‡πÅ‡∏û‡∏á", "‡∏•‡∏î", "‡∏ö‡∏≤‡∏ó", "‡∏•‡πâ‡∏≤‡∏ô", "‡πÅ‡∏™‡∏ô", "‡πÄ‡∏á‡∏¥‡∏ô", "‡πÄ‡∏™‡∏µ‡∏¢‡∏á",
                "‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà","‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô","‡∏ñ‡∏µ‡πà","‡∏ó‡∏ô","‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û","‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà","‡∏õ‡∏•‡∏≠‡∏î","‡∏£‡∏∞‡∏ö‡∏ö",
                "‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô", "‡∏ä‡∏≤‡∏£‡πå‡∏à", "‡∏Å‡∏•‡πâ‡∏≠‡∏á", "‡πÅ‡∏ö‡∏ï", "‡∏ä‡πà‡∏ß‡∏¢", "‡πÄ‡∏ó‡∏Ñ", "‡∏≠‡∏≠‡∏ü‡∏ä‡∏±‡πà‡∏ô", "‡πÑ‡∏ü‡∏ü‡πâ‡∏≤",
                "‡∏™‡∏µ", "‡∏†‡∏≤‡∏¢", "‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô", "‡∏ó‡∏£‡∏á", "‡∏£‡∏π‡∏õ", "‡∏´‡∏ô‡πâ‡∏≤", "‡∏´‡∏£‡∏π", "‡∏´‡∏£‡∏≤", "‡∏ô‡∏≠‡∏Å", "‡∏á‡∏≤‡∏°", "‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå",
                "‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏ü‡∏ü‡πâ‡∏≤", "‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà", "‡∏£‡∏π‡∏õ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå", "‡∏™‡∏°‡∏£‡∏£‡∏ñ‡∏ô‡∏∞", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"
            }
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥
            filtered_tokens = [w for w in tokens if w not in combined_stopwords and len(w) > 1 and w in allowed_words]

            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
            if not filtered_tokens:
                st.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∏‡πà‡∏ô **{selected_model}**")
            else: # ‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ filtered_tokens ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud
                wordcloud = WordCloud(
                    font_path="fonts/THSarabunNew.ttf",
                    width=800,
                    height=400,
                    background_color="white",
                    collocations=False
                ).generate(" ".join(filtered_tokens))

                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                fig, ax = plt.subplots()
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis("off")
                st.pyplot(fig)
                st.success(f"‡πÅ‡∏™‡∏î‡∏á Word Cloud ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: **{selected_model}**")

    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Word Cloud: {e}")
