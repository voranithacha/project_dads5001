import streamlit as st
from collections import Counter
import pandas as pd
from pymongo import MongoClient
import json

#for word clound
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus.common import thai_stopwords
import re

st.title("YouTube Comments Analysis")
st.markdown("""
### ðŸ”° Introduction
This project is an extension of the DADS5002 course, focusing on text classification using machine learning techniques. The objective is to automatically categorize comments extracted from YouTube videos into predefined classifications, helping to uncover insights from user feedback.
"""
"""### ðŸ” Data Source and Collection
Text Comments: Collected from YouTube comment sections of car review videos.
Target Brand: BYD, specifically its top 3 best-selling models in the Thai market â€” Atto 3, Seal, and Dolphin.
YouTube Channel: Data was gathered from the autolifethailand official channel, which has over 1.06 million subscribers and is known for its trusted automotive content.
The dataset consists of comments from three BYD-related car review clips, which serve as the foundation for building and evaluating the classification model.
"""
"""### ðŸ’¡ AI-Powered Insight Assistant
To enhance the value of this project, an AI-powered assistant feature is integrated for premium users, enabling interactive exploration of classified comments. This assistant helps identify key highlights, detect emerging themes, and provide contextual insight summaries, supporting faster and smarter decision-making based on public sentiment.
""")

# === YouTube Video IDs ===
video_ids = ["OMV9F9zB4KU", "87lJCDADWCo", "CbkX7H-0BIU"]
# === Show Videos ===
st.subheader("â–¶ï¸ Video Reference ðŸ”´")
col1, col2, col3 = st.columns(3)
with col1:
    st.video(f"https://www.youtube.com/watch?v={video_ids[0]}")
    st.caption("BYD Atto3")
with col2:
    st.video(f"https://www.youtube.com/watch?v={video_ids[1]}")
    st.caption("BYD Seal")
with col3:
    st.video(f"https://www.youtube.com/watch?v={video_ids[2]}")
    st.caption("BYD Dolphin")
# === Sidebar: Conversation History ===

# === MongoDB ===
mongo_uri = "mongodb+srv://readwrite:OSbtDM3XE8nP2JqT@voranitha.z6voe4w.mongodb.net/"
 
@st.cache_resource
def get_database():
    """
    à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸à¸±à¸š MongoDB Atlas à¹à¸¥à¸°à¸ªà¹ˆà¸‡à¸„à¸·à¸™ object à¸‚à¸­à¸‡ database
    à¹ƒà¸Šà¹‰ st.cache_resource à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹€à¸žà¸µà¸¢à¸‡à¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§à¹€à¸¡à¸·à¹ˆà¸­à¹à¸­à¸›à¸žà¸¥à¸´à¹€à¸„à¸Šà¸±à¸™à¹€à¸£à¸´à¹ˆà¸¡à¸—à¸³à¸‡à¸²à¸™
    """
    try:
        client = MongoClient(mongo_uri)
        client.admin.command('ping') # à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­
        return client.car # à¸„à¸·à¸™ database 'car'
    except ConnectionFailure as e:
        st.error(f"à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸à¸±à¸š MongoDB à¹„à¸”à¹‰: {e}")
        st.stop() # à¸«à¸¢à¸¸à¸”à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¸‚à¸­à¸‡ Streamlit à¸«à¸²à¸à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹„à¸¡à¹ˆà¹„à¸”à¹‰
    except OperationFailure as e:
        st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸‡à¸²à¸™ MongoDB: {e}")
        st.stop()
    except Exception as e:
        st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸„à¸²à¸”à¸„à¸´à¸”: {e}")
        st.stop()
 
# --- à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ Streamlit App ---
 
# à¸”à¸¶à¸‡ database object
db = get_database()
collection = db.comment
data = list(collection.find())  # Get all documents as a list of dicts
df = pd.DataFrame(data)         # Convert to DataFrame

# à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸‰à¸žà¸²à¸° video_title
#comments = list(collection.find({}, {"video_title": 1}))
#comments = list(collection.find({}, {"_id": 0, "video_title": 1}))

# à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ DataFrame
#df = pd.DataFrame(comments)
#st.write(comments[:5]) 

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ
if 'video_title' not in df.columns:
    st.error("à¹„à¸¡à¹ˆà¸žà¸šà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ 'video_title'")
else:
    # à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™à¸•à¹ˆà¸­ video_title
    video_counts = df['video_title'].value_counts().reset_index()
    video_counts.columns = ['video_title', 'count']

    # à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ªà¸£à¹‰à¸²à¸‡ label à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
    def generate_label(title):
        title_upper = title.upper()
        if "ATTO" in title_upper:
            return "BYD Atto3"
        elif "SEAL" in title_upper:
            return "BYD Seal"
        elif "DOLPHIN" in title_upper:
            return "BYD Dolphin"
        else:
            return "à¸­à¸·à¹ˆà¸™ à¹†"

    # à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ label
    video_counts['Model'] = video_counts['video_title'].apply(generate_label)

    # à¹€à¸£à¸µà¸¢à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸«à¸¡à¹ˆ
    result_df = video_counts[['Model', 'video_title', 'count']]

    # à¹à¸ªà¸”à¸‡à¸œà¸¥
    st.subheader("ðŸ“Š Video Comment Counts")
    st.dataframe(result_df)

# Word Cloud
comments_collection = db.comment

# --- à¸ªà¸£à¹‰à¸²à¸‡ Word Cloud ---
# --- UI Section ---
st.header("â˜ï¸ Word Cloud à¸ˆà¸²à¸à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™ (Filter à¸•à¸²à¸¡ Video ID)")
def clean_text(text):
    text = re.sub(r"[^\u0E00-\u0E7F]+", " ", text)  # à¹€à¸à¹‡à¸šà¹€à¸‰à¸žà¸²à¸°à¸­à¸±à¸à¸©à¸£à¹„à¸—à¸¢
    text = re.sub(r"\s+", " ", text).strip()
    return text

# à¸”à¸¶à¸‡à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸•à¸²à¸¡ video_id
comments = [doc.get("comment", "") for doc in cursor]
cleaned_comments = [clean_text(c) for c in comments]
full_text = " ".join(cleaned_comments)

# à¸•à¸±à¸”à¸„à¸³
tokens = word_tokenize(full_text, engine="newmm", keep_whitespace=False)

# à¸à¸³à¸ˆà¸±à¸” stopwords
custom_stopwords = {"à¸„à¸£à¸±à¸š", "à¸„à¹ˆà¸°", "à¹€à¸¥à¸¢", "à¹†", "à¸™à¸°", "à¸­à¹ˆà¸°", "à¸®à¸°", "à¸­à¸·à¸­"}
stopwords = set(thai_stopwords()).union(custom_stopwords)
filtered_tokens = [w for w in tokens if w not in stopwords and len(w) > 1]

# à¸ªà¸£à¹‰à¸²à¸‡ wordcloud
final_text = " ".join(filtered_tokens)
wordcloud = WordCloud(font_path="fonts/THSarabunNew.ttf", width=800, height=400).generate(final_text)
