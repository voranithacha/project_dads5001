import streamlit as st
import pandas as pd
import numpy as np
import joblib
import re
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus.common import thai_stopwords
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from keras.models import load_model
import random

# Configure page
st.set_page_config(page_title="Comment Classification", page_icon="üß†", layout="wide")
st.title("üß† Comment Classification System")

# Initialize session state
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'results_data' not in st.session_state:
    st.session_state.results_data = None

# Thai stopwords and cleaning functions
@st.cache_resource
def get_thai_stopwords():
    return set(thai_stopwords())

def remove_emojis_and_symbols(text):
    emoji_pattern = re.compile("[" 
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002700-\U000027BF"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)
    text = re.sub(r"[^\u0E00-\u0E7Fa-zA-Z0-9\s]", "", text)
    return text

def clean_text_combined(text, thai_stopwords_set):
    if isinstance(text, str):
        text = remove_emojis_and_symbols(text)
        tokens = word_tokenize(text, engine='newmm')
        seen = set()
        unique_tokens = [token for token in tokens if not (token in seen or seen.add(token))]
        filtered_tokens = [token for token in unique_tokens if token not in thai_stopwords_set and token.strip()]
        return " ".join(filtered_tokens)
    return ""

def thai_tokenizer(text):
    return word_tokenize(text, engine="newmm")

@st.cache_resource
def load_all_models():
    """Load all models and return them"""
    try:
        # Load Random Forest models
        rf_model = joblib.load('pages/final_model_rf.pkl')
        rf_vectorizer = joblib.load('pages/vectorizer_rf.pkl')
        rf_encoder = joblib.load('pages/label_encoder_rf.pkl')
        rf_selector = joblib.load('pages/selector_rf.pkl')
        rf_vectorizer.tokenizer = thai_tokenizer
        
        # Load Neural Network models
        nn_model = load_model('pages/final_model_NN.keras')
        nn_vectorizer = joblib.load('pages/vectorizer_NN.pkl')
        nn_encoder = joblib.load('pages/label_encoder_NN.pkl')
        nn_selector = joblib.load('pages/selector_NN.pkl')
        nn_vectorizer.tokenizer = thai_tokenizer
        
        return {
            'rf': {'model': rf_model, 'vectorizer': rf_vectorizer, 'encoder': rf_encoder, 'selector': rf_selector},
            'nn': {'model': nn_model, 'vectorizer': nn_vectorizer, 'encoder': nn_encoder, 'selector': nn_selector}
        }
    except Exception as e:
        st.error(f"Error loading models: {str(e)}")
        return None

def predict_with_models(texts, models, thai_stopwords_set):
    """Predict with all models and return results with probabilities"""
    results = {}
    
    # Clean texts
    cleaned_texts = [clean_text_combined(text, thai_stopwords_set) for text in texts]
    
    # Random Forest predictions
    try:
        rf_models = models['rf']
        vect_texts = rf_models['vectorizer'].transform(cleaned_texts)
        selected_features = rf_models['selector'].transform(vect_texts)
        rf_predictions = rf_models['model'].predict(selected_features)
        rf_probabilities = rf_models['model'].predict_proba(selected_features)
        rf_decoded = rf_models['encoder'].inverse_transform(rf_predictions)
        
        results['Random Forest'] = {
            'predictions': rf_decoded,
            'probabilities': rf_probabilities,
            'classes': rf_models['encoder'].classes_
        }
    except Exception as e:
        st.error(f"Error with Random Forest: {str(e)}")
    
    # Neural Network predictions
    try:
        nn_models = models['nn']
        vect_texts = nn_models['vectorizer'].transform(cleaned_texts)
        selected_features = nn_models['selector'].transform(vect_texts)
        nn_probabilities = nn_models['model'].predict(selected_features)
        nn_predictions = np.argmax(nn_probabilities, axis=1)
        nn_decoded = nn_models['encoder'].inverse_transform(nn_predictions)
        
        results['Neural Network'] = {
            'predictions': nn_decoded,
            'probabilities': nn_probabilities,
            'classes': nn_models['encoder'].classes_
        }
    except Exception as e:
        st.error(f"Error with Neural Network: {str(e)}")
    
    return results

# Main interface
col1, col2 = st.columns([1, 3])

with col1:
    st.subheader("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
    
    if st.button("üîÑ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", type="primary"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•..."):
            try:
                # Load data
                df = pd.read_csv('pages/predict_data.csv')
                
                # Load models
                models = load_all_models()
                if models is None:
                    st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ")
                    st.stop()
                
                # Get Thai stopwords
                thai_stopwords_set = get_thai_stopwords()
                
                # Predict with all models
                results = predict_with_models(df.iloc[:, 0].tolist(), models, thai_stopwords_set)
                
                # Store results in session state
                st.session_state.results_data = {
                    'original_data': df,
                    'predictions': results,
                    'texts': df.iloc[:, 0].tolist()
                }
                st.session_state.models_loaded = True
                
                st.success("‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
                
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")

with col2:
    if st.session_state.models_loaded and st.session_state.results_data:
        st.subheader("üìä ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•")
        
        display_option = st.selectbox(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•",
            ["‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô 1: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏•‡∏≤‡πÄ‡∏ö‡∏•", 
             "‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô 2: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", 
             "‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô 3: ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"]
        )
        
        results_data = st.session_state.results_data
        
        # Option 1: Show results by model and label
        if display_option == "‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô 1: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏•‡∏≤‡πÄ‡∏ö‡∏•":
            st.subheader("üéØ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏•‡∏≤‡πÄ‡∏ö‡∏•")
            
            col1_opt1, col2_opt1, col3_opt1 = st.columns(3)
            
            with col1_opt1:
                selected_model = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", list(results_data['predictions'].keys()))
            
            with col2_opt1:
                num_samples = st.number_input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á", min_value=1, max_value=len(results_data['texts']), value=10)
            
            with col3_opt1:
                # Get unique labels from selected model
                unique_labels = list(set(results_data['predictions'][selected_model]['predictions']))
                label_options = ["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"] + unique_labels
                selected_label = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡∏≤‡πÄ‡∏ö‡∏•", label_options)
            
            # Filter and display results
            model_predictions = results_data['predictions'][selected_model]['predictions']
            model_probabilities = results_data['predictions'][selected_model]['probabilities']
            
            # Create results dataframe
            results_df = pd.DataFrame({
                '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°': results_data['texts'],
                '‡∏•‡∏≤‡πÄ‡∏ö‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢': model_predictions,
                '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î': [np.max(prob) for prob in model_probabilities]
            })
            
            # Filter by label if not "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
            if selected_label != "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î":
                results_df = results_df[results_df['‡∏•‡∏≤‡πÄ‡∏ö‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢'] == selected_label]
            
            # Sample results
            if len(results_df) > num_samples:
                display_df = results_df.sample(n=num_samples).reset_index(drop=True).sort_values(by='‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î', ascending=False)
            else:
                display_df = results_df.sort_values(by='‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î', ascending=False)
            
            st.dataframe(display_df, use_container_width=True)
            st.write(f"‡πÅ‡∏™‡∏î‡∏á {len(display_df)} ‡∏à‡∏≤‡∏Å {len(results_df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # Option 2: Show consensus predictions
        elif display_option == "‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô 2: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•":
            st.subheader("ü§ù ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡πá‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô")
            
            # Find consensus predictions
            model_names = list(results_data['predictions'].keys())
            if len(model_names) >= 2:
                consensus_mask = []
                for i in range(len(results_data['texts'])):
                    predictions = [results_data['predictions'][model]['predictions'][i] for model in model_names]
                    consensus_mask.append(len(set(predictions)) == 1)  # All predictions are the same
                
                consensus_df = pd.DataFrame({
                    '‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°': [results_data['texts'][i] for i in range(len(results_data['texts'])) if consensus_mask[i]],
                    '‡∏•‡∏≤‡πÄ‡∏ö‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡πá‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô': [results_data['predictions'][model_names[0]]['predictions'][i] for i in range(len(results_data['texts'])) if consensus_mask[i]]
                })
                
                if len(consensus_df) > 0:
                    st.dataframe(consensus_df, use_container_width=True)
                    st.write(f"‡∏û‡∏ö {len(consensus_df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô")
                else:
                    st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô")
            else:
                st.warning("‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
        
        # Option 3: Show statistics charts
        elif display_option == "‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô 3: ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥":
            st.subheader("üìà ‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
            
            # 3.1 Pie charts for each model
            st.subheader("3.1 ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏≤‡πÄ‡∏ö‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
            
            cols = st.columns(len(results_data['predictions']))
            for i, (model_name, model_data) in enumerate(results_data['predictions'].items()):
                with cols[i]:
                    predictions = model_data['predictions']
                    label_counts = pd.Series(predictions).value_counts()
                    
                    fig_pie = px.pie(
                        values=label_counts.values,
                        names=label_counts.index,
                        title=f"{model_name}"
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
            
            # 3.2 Bar charts with probability breakdown
            st.subheader("3.2 ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡∏≤‡πÄ‡∏ö‡∏•‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à")
            
            for model_name, model_data in results_data['predictions'].items():
                st.write(f"**{model_name}**")
                
                predictions = model_data['predictions']
                probabilities = model_data['probabilities']
                
                # Create probability bins
                prob_bins = ['0.0-0.5', '0.5-0.7', '0.7-0.9', '0.9-1.0']
                max_probs = [np.max(prob) for prob in probabilities]
                
                # Create dataframe for plotting
                plot_data = []
                for i, (pred, max_prob) in enumerate(zip(predictions, max_probs)):
                    if max_prob < 0.5:
                        prob_bin = '0.0-0.5'
                    elif max_prob < 0.7:
                        prob_bin = '0.5-0.7'
                    elif max_prob < 0.9:
                        prob_bin = '0.7-0.9'
                    else:
                        prob_bin = '0.9-1.0'
                    
                    plot_data.append({
                        'label': pred,
                        'prob_bin': prob_bin,
                        'count': 1
                    })
                
                plot_df = pd.DataFrame(plot_data)
                pivot_df = plot_df.groupby(['label', 'prob_bin']).count().reset_index()
                pivot_df = pivot_df.pivot(index='label', columns='prob_bin', values='count').fillna(0)
                
                # Create stacked bar chart
                fig_bar = go.Figure()
                
                colors = ['#ff9999', '#ffcc99', '#99ccff', '#99ff99']
                for i, prob_bin in enumerate(prob_bins):
                    if prob_bin in pivot_df.columns:
                        fig_bar.add_trace(go.Bar(
                            name=f'‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à {prob_bin}',
                            x=pivot_df.index,
                            y=pivot_df[prob_bin],
                            marker_color=colors[i]
                        ))
                
                fig_bar.update_layout(
                    barmode='stack',
                    title=f'‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á {model_name}',
                    xaxis_title='‡∏•‡∏≤‡πÄ‡∏ö‡∏•',
                    yaxis_title='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô'
                )
                
                st.plotly_chart(fig_bar, use_container_width=True)
    else:
        st.info("üëÜ ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° '‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
