import streamlit as st
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import plotly.express as px
import duckdb as db
import numpy as np
import re
from typing import List, Tuple, Dict
import time

st.set_page_config(page_title="à¹à¸”à¸Šà¸šà¸­à¸£à¹Œà¸”à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ªà¸¶à¸", layout="wide")
st.title("ğŸ” à¹à¸”à¸Šà¸šà¸­à¸£à¹Œà¸”à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ªà¸¶à¸à¸‚à¸±à¹‰à¸™à¸ªà¸¹à¸‡ (à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™ YouTube)")

# === à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¹à¸šà¸šà¹à¸„à¸Š (à¸­à¸¢à¸¹à¹ˆà¸™à¸­à¸à¸„à¸¥à¸²à¸ªà¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™) ===
@st.cache_resource(show_spinner=True)
def load_model(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)
    return tokenizer, model

# === à¸•à¸±à¸§à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ªà¸¶à¸à¹à¸šà¸š Batch ===
class BatchSentimentAnalyzer:
    def __init__(self, model_name: str, batch_size: int = 32):
        self.model_name = model_name
        self.batch_size = batch_size
        self.tokenizer, self.model = load_model(self.model_name)
        self.labels_map = {0: "à¹€à¸Šà¸´à¸‡à¸¥à¸š", 1: "à¹€à¸›à¹‡à¸™à¸à¸¥à¸²à¸‡", 2: "à¹€à¸Šà¸´à¸‡à¸šà¸§à¸"}

    def preprocess_text(self, text: str) -> str:
        if pd.isna(text) or not isinstance(text, str):
            return ""
        return re.sub(r'\s+', ' ', text.strip())[:512]

    def predict_batch(self, texts: List[str]) -> List[Tuple[str, float]]:
        clean_texts = [self.preprocess_text(t) for t in texts]
        inputs = self.tokenizer(clean_texts, return_tensors="pt", truncation=True, padding=True, max_length=512)

        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1)
            confs = torch.max(probs, dim=1)[0]

        return [(self.labels_map[p.item()], c.item()) for p, c in zip(preds, confs)]

    def analyze_sentiments(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:
        texts = df[text_column].tolist()
        total_batches = len(texts) // self.batch_size + (1 if len(texts) % self.batch_size else 0)
        all_sentiments, all_confidences = [], []

        progress = st.progress(0)
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            results = self.predict_batch(batch)
            for sent, conf in results:
                all_sentiments.append(sent)
                all_confidences.append(conf)
            progress.progress((i + self.batch_size) / len(texts))

        df_result = df.copy()
        df_result['sentiment'] = all_sentiments
        df_result['confidence'] = all_confidences
        return df_result

# === à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ DuckDB ===
@st.cache_data
def load_data_from_duckdb(db_path: str, table_name: str) -> pd.DataFrame:
    try:
        con = db.connect(db_path)
        tables_df = con.execute("SHOW TABLES;").fetchdf()
        if table_name not in tables_df['name'].values:
            st.error(f"âŒ à¹„à¸¡à¹ˆà¸à¸šà¸•à¸²à¸£à¸²à¸‡ '{table_name}'")
            return pd.DataFrame()
        query = f"SELECT comment_text_original as comment FROM {table_name} LIMIT 1000;"
        df = con.execute(query).fetchdf()
        con.close()
        return df
    except Exception as e:
        st.error(f"âŒ à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: {str(e)}")
        return pd.DataFrame()

# === à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ ===
def analyze_text_features(df: pd.DataFrame) -> pd.DataFrame:
    df['text_length'] = df['comment'].astype(str).str.len()
    df['word_count'] = df['comment'].astype(str).str.split().str.len()
    emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿âš -âš¡]')
    df['has_emoji'] = df['comment'].astype(str).str.contains(emoji_pattern, regex=True)
    df['emoji_count'] = df['comment'].astype(str).str.count(emoji_pattern)
    df['exclamation_count'] = df['comment'].astype(str).str.count('!')
    df['question_count'] = df['comment'].astype(str).str.count('?')
    return df

# === à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ªà¸–à¸´à¸•à¸´ ===
def calculate_sentiment_statistics(df: pd.DataFrame) -> Dict:
    stats = {
        'counts': df['sentiment'].value_counts().to_dict(),
        'percentages': (df['sentiment'].value_counts(normalize=True) * 100).round(2).to_dict(),
        'avg_confidence': df['confidence'].mean(),
        'high_confidence_count': len(df[df['confidence'] > 0.8]),
        'high_confidence_percentage': (len(df[df['confidence'] > 0.8]) / len(df)) * 100
    }
    return stats

# === Main ===
def main():
    st.sidebar.header("âš™ï¸ à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²")
    db_path = st.sidebar.text_input("ğŸ—‚ à¹€à¸ªà¹‰à¸™à¸—à¸²à¸‡à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥", "./comment.duckdb")
    table_name = st.sidebar.text_input("ğŸ“„ à¸Šà¸·à¹ˆà¸­à¸•à¸²à¸£à¸²à¸‡", "yt_comment_full")
    model_name = st.sidebar.selectbox("ğŸ§  à¹€à¸¥à¸·à¸­à¸à¹‚à¸¡à¹€à¸”à¸¥", ["airesearch/wangchanberta-base-wiki", "cardiffnlp/twitter-roberta-base-sentiment-latest"])
    batch_size = st.sidebar.slider("ğŸ”¢ à¸‚à¸™à¸²à¸”à¹à¸šà¸—à¸Šà¹Œ", 8, 64, 32)

    df = load_data_from_duckdb(db_path, table_name)
    if df.empty:
        st.warning("âš ï¸ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸”à¹‰")
        return
    st.success(f"âœ… à¹‚à¸«à¸¥à¸” {len(df)} à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ªà¸³à¹€à¸£à¹‡à¸ˆ")

    analyzer = BatchSentimentAnalyzer(model_name, batch_size)

    if st.button("ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ"):
        with st.spinner("à¸à¸³à¸¥à¸±à¸‡à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ..."):
            df_result = analyzer.analyze_sentiments(df, 'comment')
            df_result = analyze_text_features(df_result)

        stats = calculate_sentiment_statistics(df_result)
        st.metric("à¸ˆà¸³à¸™à¸§à¸™à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™", len(df_result))
        st.metric("à¸„à¸§à¸²à¸¡à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸±à¹ˆà¸™à¹€à¸‰à¸¥à¸µà¹ˆà¸¢", f"{stats['avg_confidence']:.2f}")
        st.metric("à¸„à¸§à¸²à¸¡à¸„à¸´à¸”à¹€à¸«à¹‡à¸™à¸—à¸µà¹ˆà¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆà¸ªà¸¹à¸‡", f"{stats['high_confidence_percentage']:.1f}%")

        st.subheader("ğŸ“Š à¸à¸²à¸£à¸à¸£à¸°à¸ˆà¸²à¸¢à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ªà¸¶à¸")
        fig1 = px.histogram(df_result, x="sentiment", color="sentiment", title="à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ªà¸¶à¸")
        st.plotly_chart(fig1, use_container_width=True)

        st.subheader("ğŸ“‹ à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥")
        st.dataframe(df_result.head(10), use_container_width=True)

        st.subheader("â¬‡ï¸ à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸” CSV")
        csv = df_result.to_csv(index=False).encode('utf-8')
        st.download_button("ğŸ“¥ à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”", csv, "sentiment_results.csv", "text/csv")

if __name__ == "__main__":
    main()
